# RAG_NLP_MasterMLH

This repository has been created by:

* Celia de la Fuente, 
* Ana González 
* Paula Martín 
* Pablo Peiro

It contains a fully local implementation of a research assistant system based on Retrieval-Augmented Generation (RAG). It allows users to explore scientific literature by querying an indexed collection of academic abstracts and receiving relevant summaries and comparisons generated by a local LLM.

---

## 📚 Academic Research Assistant — A Multilingual RAG System

This project implements an end-to-end **academic research assistant** powered by **Retrieval-Augmented Generation (RAG)**. It enables researchers to input a research direction in **any supported language**, retrieves semantically relevant academic articles from a local vector database, and generates a comprehensive answer using an LLM. The system handles multilingual queries, LaTeX preprocessing, semantic search, and abstract summarization — all wrapped in a simple Streamlit frontend.

---

## 🧠 Key Features

- ✅ **Multilingual query support** (automatic language detection and translation)
- 📚 **Semantic search** across a cleaned and vectorized arXiv corpus
- 🧾 **Summarized academic response** using a structured prompt
- 🧠 **LLM-powered RAG backend** (using `llama3.1:8b` via Ollama)
- 📊 **Streamlit frontend** for easy use and interaction
- 📦 Efficient storage and retrieval with **ChromaDB**
- 🛠️ Modular and extensible codebase

---

## 🗂️ Project Structure

```
.
├── data/
│   ├── raw/                            # Raw unprocessed data (e.g., arXiv metadata snapshot)
│   └── processed/                      # Cleaned outputs and lookup JSONs
│       ├── arxiv_categories.json       # Mapping of arXiv category codes to names
│       └── language_dict.json          # Mapping of language codes to supported names
│
├── main/                               # Main pipeline scripts
│   ├── _1_Preprocessing_text.py        # Cleans raw metadata and generates usable CSV
│   ├── _2_Vector_database.py           # Embeds and stores documents in ChromaDB
│   ├── _3_RAG_backend.py               # Query detection, translation, search & generation
│   └── _4_RAG_frontend.py              # Streamlit frontend
│
├── utils/                              # Utility and helper scripts
│   ├── Get_categories_of_arxiv.py      # Scrapes arXiv categories from the web
│   ├── Get_languages_available.py      # Checks for languages with available translation models
│   └── Token_count.py                  # Verifies token counts against model limits
```

---

## 🔍 Overview of Each Component

### `main/_1_Preprocessing_text.py`
- Loads the massive arXiv metadata snapshot line by line
- Cleans LaTeX from titles, abstracts, and metadata
- Removes invalid rows and fills missing values
- Produces:
  - A full cleaned CSV
  - A trial subset for local testing

### `main/_2_Vector_database.py`
- Reads the cleaned corpus
- Uses [`nomic-embed-text`](https://ollama.com/library/nomic-embed-text) to generate embeddings via Ollama
- Saves vectors and metadata into a **persistent ChromaDB** database
- Avoids reprocessing already embedded documents

### `main/_3_RAG_backend.py`
- Detects query language using `langdetect`
- Translates query to English if needed (Helsinki-NLP models via Hugging Face)
- Embeds query and retrieves nearest documents (by cosine similarity)
- Filters results by category and similarity threshold
- Generates a structured response using `llama3.1:8b` via Ollama
- Translates response back to the user's language if required

### `main/_4_RAG_frontend.py`
- Built with **Streamlit**
- Allows user to:
  - Enter a research query
  - Select number of documents to retrieve
  - Filter by category
- Displays:
  - Retrieved articles (metadata, similarity score, translated abstract)
  - Final assistant answer

---

## 🛠️ Utility Scripts (`utils/`)

### `Get_categories_of_arxiv.py`
- Web scraper that pulls official arXiv category codes and names from https://arxiv.org/category_taxonomy
- Saves as `arxiv_categories.json`

### `Get_languages_available.py`
- Iterates through all `langdetect`-supported codes
- Tries to load both directions of Helsinki-NLP translation models (to/from English)
- Outputs supported languages to `language_dict.json`

### `Token_count.py`
- Uses `tiktoken` with `cl100k_base` encoding (GPT-4, GPT-3.5)
- Flags documents exceeding the token limit (8192 by default)

---

## 🧪 Requirements

Install all necessary packages using:

```bash
pip install -r requirements.txt
```

**Main dependencies:**

- `transformers`
- `torch`
- `tiktoken`
- `langdetect`
- `chromadb`
- `streamlit`
- `ollama`
- `beautifulsoup4`
- `pandas`
- `tqdm`

---

## ⚙️ How to Run the Pipeline

1. **Download and place the arXiv metadata JSON file** in `data/raw/`  
   You can get it from: https://www.kaggle.com/Cornell-University/arxiv

2. **Step 1: Preprocess the raw data**
   ```bash
   python main/_1_Preprocessing_text.py
   ```

3. **Step 2: Embed documents and store them**
   ```bash
   python main/_2_Vector_database.py
   ```

4. **Step 3: Launch the Streamlit app**
   ```bash
   streamlit run main/_4_RAG_frontend.py
   ```

> The backend is automatically used by the frontend — no need to run `_3_RAG_backend.py` directly.

---

## 🌍 Multilingual Support

The system supports any language that has both forward and backward translation models in Helsinki-NLP. A full list is generated and stored in:

```
data/processed/language_dict.json
```

If a user's query is in a supported language, the system will translate both the input and output transparently.

---

## 💡 Example Use Cases

- Exploring the state-of-the-art in a specific research area
- Comparing a new research idea with past work
- Retrieving multilingual abstracts with similarity scores
- Quickly testing viability of research directions

---

## 🙌 Acknowledgments

- [arXiv.org](https://arxiv.org) for open metadata access
- [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) for multilingual translation models
- [Ollama](https://ollama.com) for lightweight local LLM inference
- [ChromaDB](https://www.trychroma.com/) for efficient vector search


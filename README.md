# RAG_NLP_MasterMLH

This repository has been created by:

* Celia de la Fuente, 
* Ana GonzÃ¡lez 
* Paula MartÃ­n 
* Pablo Peiro

It contains a fully local implementation of a research assistant system based on Retrieval-Augmented Generation (RAG). It allows users to explore scientific literature by querying an indexed collection of academic abstracts and receiving relevant summaries and comparisons generated by a local LLM.

---

## ğŸ“š Academic Research Assistant â€” A Multilingual RAG System

This project implements an end-to-end **academic research assistant** powered by **Retrieval-Augmented Generation (RAG)**. It enables researchers to input a research direction in **any supported language**, retrieves semantically relevant academic articles from a local vector database, and generates a comprehensive answer using an LLM. The system handles multilingual queries, LaTeX preprocessing, semantic search, and abstract summarization â€” all wrapped in a simple Streamlit frontend.

---

## ğŸ§  Key Features

- âœ… **Multilingual query support** (automatic language detection and translation)
- ğŸ“š **Semantic search** across a cleaned and vectorized arXiv corpus
- ğŸ§¾ **Summarized academic response** using a structured prompt
- ğŸ§  **LLM-powered RAG backend** (using `llama3.1:8b` via Ollama)
- ğŸ“Š **Streamlit frontend** for easy use and interaction
- ğŸ“¦ Efficient storage and retrieval with **ChromaDB**
- ğŸ› ï¸ Modular and extensible codebase

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                            # Raw unprocessed data (e.g., arXiv metadata snapshot)
â”‚   â””â”€â”€ processed/                      # Cleaned outputs and lookup JSONs
â”‚       â”œâ”€â”€ arxiv_categories.json       # Mapping of arXiv category codes to names
â”‚       â””â”€â”€ language_dict.json          # Mapping of language codes to supported names
â”‚
â”œâ”€â”€ main/                               # Main pipeline scripts
â”‚   â”œâ”€â”€ _1_Preprocessing_text.py        # Cleans raw metadata and generates usable CSV
â”‚   â”œâ”€â”€ _2_Vector_database.py           # Embeds and stores documents in ChromaDB
â”‚   â”œâ”€â”€ _3_RAG_backend.py               # Query detection, translation, search & generation
â”‚   â””â”€â”€ _4_RAG_frontend.py              # Streamlit frontend
â”‚
â”œâ”€â”€ utils/                              # Utility and helper scripts
â”‚   â”œâ”€â”€ Get_categories_of_arxiv.py      # Scrapes arXiv categories from the web
â”‚   â”œâ”€â”€ Get_languages_available.py      # Checks for languages with available translation models
â”‚   â””â”€â”€ Token_count.py                  # Verifies token counts against model limits
```

---

## ğŸ” Overview of Each Component

### `main/_1_Preprocessing_text.py`
- Loads the massive arXiv metadata snapshot line by line
- Cleans LaTeX from titles, abstracts, and metadata
- Removes invalid rows and fills missing values
- Produces:
  - A full cleaned CSV
  - A trial subset for local testing

### `main/_2_Vector_database.py`
- Reads the cleaned corpus
- Uses [`nomic-embed-text`](https://ollama.com/library/nomic-embed-text) to generate embeddings via Ollama
- Saves vectors and metadata into a **persistent ChromaDB** database
- Avoids reprocessing already embedded documents

### `main/_3_RAG_backend.py`
- Detects query language using `langdetect`
- Translates query to English if needed (Helsinki-NLP models via Hugging Face)
- Embeds query and retrieves nearest documents (by cosine similarity)
- Filters results by category and similarity threshold
- Generates a structured response using `llama3.1:8b` via Ollama
- Translates response back to the user's language if required

### `main/_4_RAG_frontend.py`
- Built with **Streamlit**
- Allows user to:
  - Enter a research query
  - Select number of documents to retrieve
  - Filter by category
- Displays:
  - Retrieved articles (metadata, similarity score, translated abstract)
  - Final assistant answer

---

## ğŸ› ï¸ Utility Scripts (`utils/`)

### `Get_categories_of_arxiv.py`
- Web scraper that pulls official arXiv category codes and names from https://arxiv.org/category_taxonomy
- Saves as `arxiv_categories.json`

### `Get_languages_available.py`
- Iterates through all `langdetect`-supported codes
- Tries to load both directions of Helsinki-NLP translation models (to/from English)
- Outputs supported languages to `language_dict.json`

### `Token_count.py`
- Uses `tiktoken` with `cl100k_base` encoding (GPT-4, GPT-3.5)
- Flags documents exceeding the token limit (8192 by default)

---

## ğŸ§ª Requirements

Install all necessary packages using:

```bash
pip install -r requirements.txt
```

**Main dependencies:**

- `transformers`
- `torch`
- `tiktoken`
- `langdetect`
- `chromadb`
- `streamlit`
- `ollama`
- `beautifulsoup4`
- `pandas`
- `tqdm`

---

## âš™ï¸ How to Run the Pipeline

1. **Download and place the arXiv metadata JSON file** in `data/raw/`  
   You can get it from: https://www.kaggle.com/Cornell-University/arxiv

2. **Step 1: Preprocess the raw data**
   ```bash
   python main/_1_Preprocessing_text.py
   ```

3. **Step 2: Embed documents and store them**
   ```bash
   python main/_2_Vector_database.py
   ```

4. **Step 3: Launch the Streamlit app**
   ```bash
   streamlit run main/_4_RAG_frontend.py
   ```

> The backend is automatically used by the frontend â€” no need to run `_3_RAG_backend.py` directly.

---

## ğŸŒ Multilingual Support

The system supports any language that has both forward and backward translation models in Helsinki-NLP. A full list is generated and stored in:

```
data/processed/language_dict.json
```

If a user's query is in a supported language, the system will translate both the input and output transparently.

---

## ğŸ’¡ Example Use Cases

- Exploring the state-of-the-art in a specific research area
- Comparing a new research idea with past work
- Retrieving multilingual abstracts with similarity scores
- Quickly testing viability of research directions

---

## ğŸ™Œ Acknowledgments

- [arXiv.org](https://arxiv.org) for open metadata access
- [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) for multilingual translation models
- [Ollama](https://ollama.com) for lightweight local LLM inference
- [ChromaDB](https://www.trychroma.com/) for efficient vector search

